#!/usr/bin/env python3
import os
import io
import json
import sqlite3
import shutil
import subprocess
import time
import urllib.parse
import requests
import zipfile
import datetime
from packaging.version import parse as parse_version
from dotenv import load_dotenv
from slack_sdk import WebClient
from slack_sdk.errors import SlackApiError

# Import OpenAI library
from openai import OpenAI

#############################
# Configuration & Constants #
#############################

load_dotenv(override=True)

# Environment variables
BITBUCKET_WORKSPACE = os.getenv("BITBUCKET_WORKSPACE")
ATLASSIAN_USERNAME = os.getenv("ATLASSIAN_USERNAME")
ATLASSIAN_API_KEY = os.getenv("ATLASSIAN_API_KEY")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
NVD_API_KEY = os.getenv("NVD_API_KEY")
SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")
SLACK_API_TOKEN = os.getenv("SLACK_API_TOKEN")
SLACK_CHANNEL_ID=os.getenv("SLACK_CHANNEL_ID")


# Use GPT model with web search if available (adjust if needed)
GPT_MODEL_WITH_SEARCH = "gpt-3.5-turbo"

if not all([BITBUCKET_WORKSPACE, ATLASSIAN_USERNAME, ATLASSIAN_API_KEY,
            OPENAI_API_KEY, NVD_API_KEY, SLACK_WEBHOOK_URL, SLACK_API_TOKEN]):
    raise Exception("One or more required environment variables are missing.")

client = OpenAI(api_key=OPENAI_API_KEY)

# Directories & file paths
REPOS_DIR = "repos"
SBOMS_DIR = "sboms"
DB_FILE = "repo_vuln.db"
OPENAI_LOG_FILE = "openai_logs.txt"
OUTPUT_AGGREGATED_FILE = "aggregated_vulnerabilities.json"

# NVD feeds URLs and paths
NVD_CVE_MODIFIED_ZIP_URL = "https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-modified.json.zip"
NVD_CVE_RECENT_ZIP_URL   = "https://nvd.nist.gov/feeds/json/cve/1.1/nvdcve-1.1-recent.json.zip"
NVD_CVE_FEED_FOLDER = "nvdcve_feed"
NVD_CVE_MODIFIED_JSON = os.path.join(NVD_CVE_FEED_FOLDER, "nvdcve-1.1-modified.json")
NVD_CVE_RECENT_JSON   = os.path.join(NVD_CVE_FEED_FOLDER, "nvdcve-1.1-recent.json")

# Vulnerability API endpoint (Shodan CVE DB; no header used)
VULN_API_BASE = "https://cvedb.shodan.io/cves"

# NVD REST API details (headers required)
NVD_REST_API_BASE = "https://services.nvd.nist.gov/rest/json/cves/2.0?cveId="
NVD_API_HEADERS = {"apiKey": NVD_API_KEY}

REQUEST_DELAY = 1
ONE_DAY_SECONDS = 86400

#############################
# Slack Helpers             #
#############################

def send_slack_file(filepath, initial_comment=None, token=None, channel_id=None):
    """
    Executes a command to upload a file to Slack using an external Python script.
    It runs the exact command:
    
    python file_upload.py --token="" --file="./aggregated_vulnerabilities.json" --channels="token"
    
    Args:
        filepath (str): Path to the file to upload.
    """
    command = [
        "python3.9", "file_upload.py",
        '--token="{SLACK_API_TOKEN}"'.format(SLACK_API_TOKEN=SLACK_API_TOKEN),
        '--file="{filepath}"'.format(filepath=filepath),
        '--channels="{SLACK_CHANNEL_ID}"'.format(SLACK_CHANNEL_ID=SLACK_CHANNEL_ID)
    ]
    try:
        subprocess.run(" ".join(command), shell=True, check=True)
        print("File uploaded to Slack successfully.")
    except subprocess.CalledProcessError as e:
        print(f"Error uploading file via command: {e}")

def send_slack_message(text):
    """
    Sends a simple text message to Slack via the webhook.
    Updated to use mrkdwn = True for proper markdown rendering.
    """
    payload = {
        "text": text,
        "mrkdwn": True
    }
    try:
        requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=10).raise_for_status()
    except requests.RequestException as e:
        print(f"Slack send error: {e}")

#############################
# OpenAI Logging Helpers    #
#############################

def log_openai_interaction(prompt, response_text):
    with open(OPENAI_LOG_FILE, "a", encoding="utf-8") as f:
        f.write("\n------------------------\n")
        f.write(f"Timestamp: {datetime.datetime.utcnow().isoformat()}Z\n")
        f.write("Prompt:\n")
        f.write(prompt + "\n\n")
        f.write("Response:\n")
        f.write(response_text + "\n")

def log_and_return_openai(prompt, content):
    log_openai_interaction(prompt, content)
    return content

#############################
# Code Fence Stripping      #
#############################

def strip_code_fences(text: str) -> str:
    lines = text.strip().splitlines()
    if lines and lines[0].strip().startswith("```"):
        lines = lines[1:]
    if lines and lines[-1].strip().startswith("```"):
        lines = lines[:-1]
    return "\n".join(lines).strip()

def parse_model_json(response_text: str):
    cleaned = strip_code_fences(response_text)
    try:
        return json.loads(cleaned)
    except json.JSONDecodeError as e:
        print(f"OpenAI JSON parse error: {e}")
        return None

#############################
# Utility Functions         #
#############################

def download_and_extract(url, extract_folder):
    print(f"[{datetime.datetime.now()}] Downloading {url} ...")
    response = requests.get(url, timeout=60)
    response.raise_for_status()
    zip_data = io.BytesIO(response.content)
    os.makedirs(extract_folder, exist_ok=True)
    with zipfile.ZipFile(zip_data) as z:
        z.extractall(extract_folder)
    print(f"[{datetime.datetime.now()}] Extraction complete to folder '{extract_folder}'.")

def refresh_feed_if_old(feed_file, feed_url):
    if os.path.exists(feed_file):
        mtime = os.path.getmtime(feed_file)
        if time.time() - mtime > ONE_DAY_SECONDS:
            print(f"[{datetime.datetime.now()}] {feed_file} is older than one day. Attempting to refresh feed...")
            try:
                download_and_extract(feed_url, os.path.dirname(feed_file))
            except Exception as e:
                print(f"[{datetime.datetime.now()}] Failed to refresh feed. Using existing feed. Error: {e}")
        else:
            print(f"[{datetime.datetime.now()}] {feed_file} is fresh.")
    else:
        print(f"[{datetime.datetime.now()}] {feed_file} not found. Attempting to download feed...")
        try:
            download_and_extract(feed_url, os.path.dirname(feed_file))
        except Exception as e:
            print(f"[{datetime.datetime.now()}] Failed to download feed. No local copy available. Error: {e}")

def load_json_file(file_path):
    with open(file_path, "r") as f:
        return json.load(f)

def query_nvd_rest_api(cve_id):
    url = f"{NVD_REST_API_BASE}{urllib.parse.quote(cve_id)}"
    try:
        r = requests.get(url, headers=NVD_API_HEADERS, timeout=30)
        r.raise_for_status()
        return r.json()
    except requests.RequestException as e:
        print(f"Error querying NVD REST API for {cve_id}: {e}")
        return None

def version_in_range(version, start_including=None, end_including=None, start_excluding=None, end_excluding=None):
    try:
        ver = parse_version(version)
    except Exception:
        return False
    if start_including and ver < parse_version(start_including):
        return False
    if start_excluding and ver <= parse_version(start_excluding):
        return False
    if end_including and ver > parse_version(end_including):
        return False
    if end_excluding and ver >= parse_version(end_excluding):
        return False
    return True

def parse_cpe_from_configuration(cpe_criteria):
    parts = cpe_criteria.split(":")
    if len(parts) < 6:
        return None, None, None
    return parts[3], parts[4], parts[5]

def parse_cpe_string(cpe_str):
    parts = cpe_str.split(":")
    if len(parts) < 6:
        return None, None, None
    return parts[3], parts[4], parts[5]

#############################
# Config File Scanner       #
#############################

def find_config_files(repo_path, extensions=(".yaml", ".yml", ".json", ".ini", ".cfg")):
    config_files = []
    for root, dirs, files in os.walk(repo_path):
        for file in files:
            if file.lower().endswith(extensions):
                config_files.append(os.path.join(root, file))
    return config_files

def local_read_file(filepath):
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            return f.read()
    except Exception as e:
        print(f"Error reading {filepath}: {e}")
        return ""

def chunk_text(text, max_chars=2000):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + max_chars, len(text))
        chunks.append(text[start:end])
        start = end
    return chunks

def call_model_for_websearch(chunk):
    prompt = f"""You have web search capabilities.
Given this chunk of a configuration file:

{chunk}

Identify any software dependencies including vendor, product, and version.
Return a JSON array with objects having keys "vendor", "product", and "version".
Return only JSON.
"""
    try:
        resp = client.chat.completions.create(
            model=GPT_MODEL_WITH_SEARCH,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0
        )
        raw_content = resp.choices[0].message.content.strip()
        log_openai_interaction(prompt, raw_content)
        return parse_model_json(raw_content)
    except Exception as e:
        print(f"OpenAI call error: {e}")
        return None

def process_config_file(filepath):
    text = local_read_file(filepath)
    if not text:
        return []
    chunks = chunk_text(text)
    aggregated = []
    for c in chunks:
        data = call_model_for_websearch(c)
        if data and isinstance(data, list):
            aggregated.extend(data)
        time.sleep(REQUEST_DELAY)
    return aggregated

#############################
# Shodan CVE DB Queries     #
#############################

def query_vuln_api_by_cpe(cpe):
    encoded_cpe = urllib.parse.quote(cpe, safe="")
    url = f"{VULN_API_BASE}?cpe23={encoded_cpe}&count=false&is_kev=false&sort_by_epss=false&skip=0&limit=1000"
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        return r.json()
    except requests.RequestException as e:
        print(f"Error querying vulnerability API for CPE '{cpe}': {e}")
        return None

def query_vuln_api_by_product(product):
    encoded_product = urllib.parse.quote(product, safe="")
    url = f"{VULN_API_BASE}?product={encoded_product}&skip=0&limit=1000&count=false&is_kev=false&sort_by_epss=false"
    try:
        r = requests.get(url, timeout=30)
        r.raise_for_status()
        return r.json()
    except requests.RequestException as e:
        print(f"Error querying vulnerability API for product '{product}': {e}")
        return None

#############################
# OpenAI-based Helpers      #
#############################

def get_correct_cpe_details(raw_cpe):
    prompt = (
        f"Extract the canonical vendor, product, and version from the following CPE string:\n"
        f"{raw_cpe}\n"
        "Return only a JSON object with keys 'vendor', 'product', and 'version'."
    )
    try:
        resp = client.chat.completions.create(
            model=GPT_MODEL_WITH_SEARCH,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,
            temperature=0
        )
        content = resp.choices[0].message.content.strip()
        content = log_and_return_openai(prompt, content)
        data = json.loads(strip_code_fences(content))
        return data.get("vendor"), data.get("product"), data.get("version")
    except Exception as e:
        print(f"OpenAI error for CPE '{raw_cpe}': {e}")
        return None, None, None

def extract_cpe_info_from_cve_llm(cve_obj):
    description_text = ""
    if "descriptions" in cve_obj:
        for desc in cve_obj["descriptions"]:
            if desc.get("lang") == "en":
                description_text = desc.get("value", "")
                break
    if not description_text:
        description_text = cve_obj.get("summary", "")
    if not description_text:
        return None, None, None
    prompt = (
        f"Extract the canonical vendor, product, and version from the following CVE description:\n"
        f"{description_text}\n"
        "Return a JSON object with keys 'vendor', 'product', and 'version'."
    )
    try:
        resp = client.chat.completions.create(
            model=GPT_MODEL_WITH_SEARCH,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=60,
            temperature=0
        )
        content = resp.choices[0].message.content.strip()
        content = log_and_return_openai(prompt, content)
        data = json.loads(strip_code_fences(content))
        return data.get("vendor"), data.get("product"), data.get("version")
    except Exception as e:
        print(f"OpenAI error extracting CPE from CVE: {e}")
        return None, None, None

def openai_validate_vulnerability(vuln_details, vendor, product, version, repo_version):
    prompt = (
        f"Vendor: {vendor}\nProduct: {product}\nReported Version: {version}\n"
        f"Repository Version: {repo_version}\nSummary: {vuln_details.get('summary', '')}\n"
        f"References: {', '.join(vuln_details.get('references', []))}\n\n"
        "Does the repository version fall within the affected range? "
        "Respond with 'Match' or 'No Match' plus a brief explanation."
    )
    try:
        resp = client.chat.completions.create(
            model=GPT_MODEL_WITH_SEARCH,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=50,
            temperature=0
        )
        content = resp.choices[0].message.content.strip()
        return log_and_return_openai(prompt, content)
    except Exception as e:
        return f"OpenAI error: {e}"

def openai_detailed_vulnerability_assessment(vuln_info):
    actual_severity = vuln_info.get("severity", "Unknown")
    cvss = vuln_info.get("cvss")
    epss = vuln_info.get("epss")
    description = vuln_info.get("description", "")
    references = vuln_info.get("references", [])
    vuln_data = {
        "actual_severity": actual_severity,
        "cvss": cvss,
        "epss": epss,
        "description": description,
        "references": references
    }
    vuln_data_str = json.dumps(vuln_data, indent=2)
    prompt = (
        "You are a cybersecurity expert with web search capabilities. Analyze the following vulnerability data:\n"
        f"{vuln_data_str}\n\n"
        "Provide a detailed assessment that includes:\n"
        "  - detailed_description (explain the vulnerability and its real-life impact)\n"
        "  - vulnerable_package (the name of the affected package)\n"
        "  - mitigation (recommended remediation steps)\n"
        "  - predicted_severity (final severity rating: Low, Medium, High, or Critical)\n"
        "  - predicted_score (numerical score 0-10 reflecting risk)\n"
        "  - explanation (brief reasoning behind your assessment)\n"
        "Return the result as a JSON object with the above keys."
    )
    try:
        resp = client.chat.completions.create(
            model=GPT_MODEL_WITH_SEARCH,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=250,
            temperature=0
        )
        content = resp.choices[0].message.content.strip()
        log_openai_interaction(prompt, content)
        data = parse_model_json(content)
        if not data or not isinstance(data, dict):
            data = {
                "predicted_severity": "Unknown",
                "predicted_score": 0.0,
                "detailed_description": "",
                "vulnerable_package": "",
                "mitigation": "",
                "explanation": "OpenAI returned invalid or empty JSON"
            }
        return data
    except Exception as e:
        print(f"OpenAI error in detailed assessment: {e}")
        return {
            "predicted_severity": "Unknown",
            "predicted_score": 0.0,
            "detailed_description": "",
            "vulnerable_package": "",
            "mitigation": "",
            "explanation": f"OpenAI error: {e}"
        }

#############################
# Database Functions        #
#############################

def connect_db():
    return sqlite3.connect(DB_FILE)

def init_db_all():
    conn = connect_db()
    cur = conn.cursor()
    cur.execute("""
        CREATE TABLE IF NOT EXISTS repositories (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT UNIQUE,
            clone_url TEXT,
            last_commit TEXT,
            last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
    """)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS repository_cpes (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            repository_id INTEGER,
            cpe TEXT,
            vendor TEXT,
            product TEXT,
            version TEXT,
            UNIQUE(repository_id, cpe),
            FOREIGN KEY(repository_id) REFERENCES repositories(id)
        )
    """)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS grype_vulnerabilities (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            repository_name TEXT,
            vulnerability_id TEXT,
            actual_severity TEXT,
            predicted_severity TEXT,
            predicted_score REAL,
            detailed_description TEXT,
            vulnerable_package TEXT,
            mitigation TEXT,
            vulnerability_json TEXT,
            query_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(repository_name, vulnerability_id)
        )
    """)
    cur.execute("""
        CREATE TABLE IF NOT EXISTS nvd_vulnerabilities (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            repository_id INTEGER,
            cve_id TEXT,
            cpe TEXT,
            vulnerability_json TEXT,
            query_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(repository_id, cve_id),
            FOREIGN KEY(repository_id) REFERENCES repositories(id)
        )
    """)
    conn.commit()
    return conn

def get_repository_record(conn, repo_name):
    cur = conn.cursor()
    cur.execute("SELECT id, last_commit FROM repositories WHERE name = ?", (repo_name,))
    return cur.fetchone()

def upsert_repository(conn, repo_name, clone_url, last_commit):
    cur = conn.cursor()
    record = get_repository_record(conn, repo_name)
    if record:
        repo_id, old_commit = record
        if old_commit != last_commit:
            cur.execute("""
                UPDATE repositories
                SET clone_url = ?, last_commit = ?, last_updated = CURRENT_TIMESTAMP
                WHERE id = ?
            """, (clone_url, last_commit, repo_id))
            conn.commit()
            # send_slack_message(f":arrows_clockwise: *{repo_name}* updated with new commit `{last_commit}`.")
        return repo_id
    else:
        cur.execute("""
            INSERT INTO repositories (name, clone_url, last_commit)
            VALUES (?, ?, ?)
        """, (repo_name, clone_url, last_commit))
        conn.commit()
        new_id = cur.lastrowid
        # send_slack_message(f":new: New repository *{repo_name}* added with commit `{last_commit}`.")
        return new_id

def update_repository_cpes(conn, repo_id, cpe_set):
    cur = conn.cursor()
    cur.execute("DELETE FROM repository_cpes WHERE repository_id = ?", (repo_id,))
    for cpe in cpe_set:
        vendor, product, version = parse_cpe_string(cpe)
        cur.execute("""
            INSERT OR IGNORE INTO repository_cpes (repository_id, cpe, vendor, product, version)
            VALUES (?, ?, ?, ?, ?)
        """, (repo_id, cpe, vendor, product, version))
    conn.commit()

def fetch_repository_cpe_records(conn):
    cur = conn.cursor()
    cur.execute("SELECT id, repository_id, cpe, vendor, product, version FROM repository_cpes")
    return cur.fetchall()

def fetch_all_repository_records(conn):
    cur = conn.cursor()
    cur.execute("SELECT id, name FROM repositories")
    return cur.fetchall()

#############################
# Bitbucket & SBOM Functions#
#############################

def fetch_all_repositories(workspace, username, api_key):
    base_url = f"https://api.bitbucket.org/2.0/repositories/{workspace}"
    all_repos = []
    url = base_url
    while url:
        resp = requests.get(url, auth=(username, api_key))
        if resp.status_code != 200:
            print(f"Error fetching repos: {resp.status_code} - {resp.text}")
            break
        data = resp.json()
        all_repos.extend(data.get("values", []))
        url = data.get("next")
    return all_repos

def extract_repo_info(repo):
    name = repo.get("name")
    clone_links = repo.get("links", {}).get("clone", [])
    ssh_url = None
    for link in clone_links:
        if link.get("name") == "ssh":
            ssh_url = link.get("href")
            break
    if not ssh_url:
        for link in clone_links:
            if link.get("name") == "https":
                ssh_url = link.get("href")
                break
    return {"name": name, "clone_url": ssh_url}

def get_remote_head_commit(clone_url):
    try:
        res = subprocess.run(["git", "ls-remote", clone_url, "HEAD"],
                             stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True, check=True)
        line = res.stdout.strip()
        commit_hash = line.split()[0] if line else None
        return commit_hash
    except subprocess.CalledProcessError:
        return None

def clone_repository(repo_name, clone_url):
    repo_path = os.path.join(REPOS_DIR, repo_name)
    if os.path.exists(repo_path):
        shutil.rmtree(repo_path)
    print(f"[{datetime.datetime.now()}] Cloning repository '{repo_name}'...")
    subprocess.check_call(["git", "clone", clone_url, repo_path])
    return repo_path

def generate_sbom_syft(repo_path, repo_name):
    sbom_file = os.path.join(SBOMS_DIR, f"{repo_name}.json")
    print(f"[{datetime.datetime.now()}] Generating SBOM for '{repo_name}' with Syft...")
    cmd = ["syft", "scan", f"dir:{repo_path}", "-o", "cyclonedx-json"]
    with open(sbom_file, "w") as outfile:
        subprocess.check_call(cmd, stdout=outfile)
    # send_slack_message(f":gear: SBOM generated for *{repo_name}*.")
    return sbom_file

def delete_repository(repo_path, repo_name):
    print(f"[{datetime.datetime.now()}] Deleting local clone of '{repo_name}'.")
    shutil.rmtree(repo_path)

#############################
# Grype Vulnerability Scan  #
#############################

def run_grype_on_sbom(sbom_filepath):
    try:
        cmd = f"cat {sbom_filepath} | grype --by-cve -o json"
        res = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE,
                             stderr=subprocess.PIPE, text=True, check=True)
        output = res.stdout.strip()
        if output:
            return json.loads(output)
    except subprocess.CalledProcessError:
        return None
    return None

def process_grype_vulnerabilities():
    new_vulns = []
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()

    sbom_files = [f for f in os.listdir(SBOMS_DIR) if f.lower().endswith(".json")]
    for sbom_file in sbom_files:
        repo_name = os.path.splitext(sbom_file)[0]
        sbom_path = os.path.join(SBOMS_DIR, sbom_file)
        print(f"[{datetime.datetime.now()}] Running Grype on SBOM for repository '{repo_name}'...")
        grype_data = run_grype_on_sbom(sbom_path)
        if not grype_data:
            print(f"  No Grype output for {repo_name}.")
            continue

        matches = grype_data.get("matches", [])
        print(f"  Found {len(matches)} vulnerabilities in Grype scan for {repo_name}.")
        for match in matches:
            vuln_info = match.get("vulnerability", {})
            vulnerability_id = vuln_info.get("id")
            if not vulnerability_id:
                continue

            # Check if vulnerability is already recorded
            cur.execute("""
                SELECT id FROM grype_vulnerabilities
                WHERE repository_name = ? AND vulnerability_id = ?
            """, (repo_name, vulnerability_id))
            if cur.fetchone():
                continue  # Already recorded

            actual_sev = vuln_info.get("severity", "Unknown")
            detailed_assessment = openai_detailed_vulnerability_assessment(vuln_info)
            predicted_sev = detailed_assessment.get("predicted_severity", "Unknown")
            predicted_score = detailed_assessment.get("predicted_score", 0.0)
            explanation = detailed_assessment.get("explanation", "")
            detailed_description = detailed_assessment.get("detailed_description", "")
            vulnerable_package = detailed_assessment.get("vulnerable_package", "")
            mitigation = detailed_assessment.get("mitigation", "")


            # Insert the new vulnerability in DB
            cur.execute("""
                INSERT INTO grype_vulnerabilities (
                    repository_name, vulnerability_id,
                    actual_severity, predicted_severity, predicted_score,
                    detailed_description, vulnerable_package, mitigation,
                    vulnerability_json
                )
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                repo_name, vulnerability_id,
                actual_sev, predicted_sev, predicted_score,
                detailed_description, vulnerable_package, mitigation,
                json.dumps(vuln_info)
            ))
            conn.commit()

            # Attempt to retrieve vendor/product/version from repository_cpes if there's a known cpe
            # For demonstration, let's assume 'artifact' might contain cpe:
            artifact_cpe = match.get("artifact", {}).get("cpe", "")
            vendor_db = "N/A"
            product_db = "N/A"
            version_db = "N/A"
            if artifact_cpe:
                cur.execute("""
                    SELECT vendor, product, version
                    FROM repository_cpes
                    WHERE repository_id IN (
                        SELECT id FROM repositories WHERE name = ?
                    )
                    AND cpe = ?
                """, (repo_name, artifact_cpe))
                row = cur.fetchone()
                if row:
                    vendor_db, product_db, version_db = row

            new_vulns.append({
                "repository": repo_name,
                "vulnerability_id": vulnerability_id,
                "actual_severity": actual_sev,
                "predicted_severity": predicted_sev,
                "predicted_score": predicted_score,
                "detailed_description": detailed_description,
                "vulnerable_package": vulnerable_package,
                "mitigation": mitigation,
                # newly added vendor info for Slack
                "vendor": vendor_db,
                "product": product_db,
                "version": version_db
            })
            print(f"  New vulnerability {vulnerability_id} recorded for {repo_name}.")

        time.sleep(REQUEST_DELAY)
    conn.close()
    return new_vulns

#############################
# NVD CVE Feed Processing   #
#############################

def process_nvd_feeds(conn):
    refresh_feed_if_old(NVD_CVE_MODIFIED_JSON, NVD_CVE_MODIFIED_ZIP_URL)
    refresh_feed_if_old(NVD_CVE_RECENT_JSON, NVD_CVE_RECENT_ZIP_URL)

    modified_feed = load_json_file(NVD_CVE_MODIFIED_JSON)
    recent_feed = load_json_file(NVD_CVE_RECENT_JSON)
    cve_items = modified_feed.get("CVE_Items", []) + recent_feed.get("CVE_Items", [])
    print(f"[{datetime.datetime.now()}] Processing {len(cve_items)} CVE items from NVD feeds.")

    cur = conn.cursor()
    repo_cpe_recs = fetch_repository_cpe_records(conn)
    repo_id_to_name = {r[0]: r[1] for r in fetch_all_repository_records(conn)}

    new_count = 0
    for item in cve_items:
        cve = item.get("cve", {})
        cve_id = cve.get("CVE_data_meta", {}).get("ID")
        if not cve_id:
            continue

        cve_details = query_nvd_rest_api(cve_id) or cve
        configs = item.get("configurations", {}).get("nodes", [])
        if not configs:
            llm_vendor, llm_product, llm_version = extract_cpe_info_from_cve_llm(cve.get("descriptions", {}) or cve)
            if llm_vendor and llm_product:
                config_entries = [{"criteria": f"cpe:2.3:a:{llm_vendor}:{llm_product}:{llm_version}:*:*:*:*:*:*:*"}]
            else:
                config_entries = []
        else:
            config_entries = []
            for node in configs:
                for cpe_match in node.get("cpe_match", []):
                    if "criteria" in cpe_match:
                        config_entries.append(cpe_match)

        for conf in config_entries:
            criteria = conf.get("criteria")
            if not criteria:
                continue
            v, p, ver = parse_cpe_from_configuration(criteria)
            c_v, c_p, c_ver = get_correct_cpe_details(criteria)
            if c_v and c_p:
                v, p, ver = c_v, c_p, c_ver

            for rec in repo_cpe_recs:
                rec_id, repository_id, db_cpe, db_vendor, db_product, db_version = rec

                if db_cpe == criteria:
                    match_type = "raw_cpe_match"
                elif (db_vendor and db_product and v and p and
                      db_vendor.lower() == v.lower() and db_product.lower() == p.lower()):
                    match_type = "vendor_product_match"
                else:
                    continue

                version_valid = True
                if any(k in conf for k in ["versionStartIncluding","versionEndIncluding",
                                           "versionStartExcluding","versionEndExcluding"]):
                    version_valid = version_in_range(
                        db_version,
                        start_including=conf.get("versionStartIncluding"),
                        end_including=conf.get("versionEndIncluding"),
                        start_excluding=conf.get("versionStartExcluding"),
                        end_excluding=conf.get("versionEndExcluding")
                    )
                if not version_valid:
                    continue

                _ = openai_validate_vulnerability(cve_details, v, p, ver, db_version)

                try:
                    cur.execute("""
                        INSERT OR IGNORE INTO nvd_vulnerabilities (repository_id, cve_id, cpe, vulnerability_json)
                        VALUES (?, ?, ?, ?)
                    """, (repository_id, cve_id, criteria, json.dumps(cve_details)))
                    conn.commit()
                    if cur.rowcount:
                        new_count += 1
                        print(f"Inserted new NVD CVE {cve_id} for repository {repo_id_to_name.get(repository_id, 'Unknown')}.")
                except Exception as e:
                    repo_name = repo_id_to_name.get(repository_id, "Unknown")
                    print(f"Error inserting CVE {cve_id} for repository {repo_name}: {e}")

        time.sleep(REQUEST_DELAY)

    print(f"{new_count} new NVD CVE records inserted.")
    return new_count

#############################
# Aggregation & Slack       #
#############################

def aggregate_vulnerabilities():
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()

    # Collect from Grype vulnerabilities
    cur.execute("""
        SELECT repository_name, vulnerability_id, actual_severity,
               predicted_severity, predicted_score, detailed_description,
               vulnerable_package, mitigation
        FROM grype_vulnerabilities
    """)
    grype_data = cur.fetchall()
    grype_vulns = []
    for g in grype_data:
        repo, vuln_id, actual, predicted, score, desc, pkg, mit = g
        grype_vulns.append({
            "repository": repo,
            "vulnerability_id": vuln_id,
            "actual_severity": actual,
            "predicted_severity": predicted,
            "predicted_score": score,
            "detailed_description": desc,
            "vulnerable_package": pkg,
            "mitigation": mit
        })

    # Collect from NVD vulnerabilities
    cur.execute("""
        SELECT nvd_vulnerabilities.repository_id, cve_id, cpe, vulnerability_json
        FROM nvd_vulnerabilities
    """)
    nvd_data = cur.fetchall()
    repo_lookup = {r[0]: r[1] for r in fetch_all_repository_records(conn)}
    nvd_vulns = []
    for row in nvd_data:
        repo_id, cve_id, cpe, vuln_json = row
        repo_name = repo_lookup.get(repo_id, "Unknown")
        nvd_vulns.append({
            "repository": repo_name,
            "vulnerability_id": cve_id,
            "actual_severity": "N/A",
            "predicted_severity": "N/A",
            "predicted_score": 0.0,
            "detailed_description": "",
            "vulnerable_package": "",
            "mitigation": ""
        })

    conn.close()

    all_vulns = grype_vulns + nvd_vulns
    sorted_vulns = sorted(all_vulns, key=lambda x: x.get("predicted_score", 0), reverse=True)
    with open(OUTPUT_AGGREGATED_FILE, "w") as f:
        json.dump(sorted_vulns, f, indent=4)
    return sorted_vulns

def send_aggregated_slack_file():
    aggregated = aggregate_vulnerabilities()
    if aggregated:
        msg = ":warning: *Aggregated Vulnerability Report* (Top high-priority items attached)"
        send_slack_file(OUTPUT_AGGREGATED_FILE, msg)
    else:
        send_slack_message("No new high-priority vulnerabilities detected.")

#############################
# Repository Processing     #
#############################

def process_repositories():
    """
    Clones or updates each repository if a new commit is detected, then:
      - Generate SBOM
      - Scan configuration files (for dependency information) â€“ if repository exists already (not first run)
      - Combine SBOM CPEs and (if available) config file CPEs
      - Store results in the database
    """
    conn = sqlite3.connect(DB_FILE)
    repos_raw = fetch_all_repositories(BITBUCKET_WORKSPACE, ATLASSIAN_USERNAME, ATLASSIAN_API_KEY)
    repos = [extract_repo_info(r) for r in repos_raw]
    print(f"[{datetime.datetime.now()}] Fetched {len(repos)} repositories from workspace '{BITBUCKET_WORKSPACE}'.")

    for repo in repos:
        repo_name = repo.get("name")
        clone_url = repo.get("clone_url")
        if not repo_name or not clone_url:
            continue

        print(f"[{datetime.datetime.now()}] Processing repository: {repo_name}")
        remote_commit = get_remote_head_commit(clone_url)
        if not remote_commit:
            print(f"  Could not fetch remote commit for '{repo_name}'. Skipping.")
            continue

        cur = conn.cursor()
        cur.execute("SELECT id, last_commit FROM repositories WHERE name = ?", (repo_name,))
        record = cur.fetchone()

        sbom_file = os.path.join(SBOMS_DIR, f"{repo_name}.json")
        needs_new_sbom = True
        if record and record[1] == remote_commit and os.path.exists(sbom_file):
            print(f"  No code changes for '{repo_name}'. Using existing SBOM.")
            needs_new_sbom = False
        else:
            print(f"  New commit detected for '{repo_name}': {remote_commit}")

        repo_path = None
        if needs_new_sbom:
            try:
                repo_path = clone_repository(repo_name, clone_url)
                sbom_file = generate_sbom_syft(repo_path, repo_name)
            except subprocess.CalledProcessError as e:
                print(f"  Error processing '{repo_name}': {e}")
                continue
        else:
            # If no new commit, attempt to use an existing local clone (if available)
            local_repo_path = os.path.join(REPOS_DIR, repo_name)
            if os.path.exists(local_repo_path):
                repo_path = local_repo_path

        # Parse SBOM for CPEs
        sbom_cpes = set()
        if os.path.exists(sbom_file):
            try:
                with open(sbom_file, "r", encoding="utf-8") as f:
                    sbom_data = json.load(f)
                for comp in sbom_data.get("components", []):
                    if comp.get("cpe"):
                        sbom_cpes.add(comp["cpe"])
                    for prop in comp.get("properties", []):
                        if prop.get("name") == "syft:cpe23" and prop.get("value"):
                            sbom_cpes.add(prop["value"])
            except Exception as e:
                print(f"  Error loading SBOM for '{repo_name}': {e}")
        print(f"  SBOM CPE count for '{repo_name}': {len(sbom_cpes)}")

        # Process configuration files ONLY if repository record already exists (i.e. not first run)
        config_cpes = set()
        if repo_path and os.path.exists(repo_path) and record:
            config_files = find_config_files(repo_path)
            for cfg_file in config_files:
                results = process_config_file(cfg_file)
                if results and isinstance(results, list):
                    for r in results:
                        vendor = r.get("vendor")
                        product = r.get("product")
                        version = r.get("version")
                        if vendor and product and version:
                            cpe = f"cpe:2.3:a:{vendor}:{product}:{version}:*:*:*:*:*:*:*"
                            config_cpes.add(cpe)
            print(f"  Config file CPE count for '{repo_name}': {len(config_cpes)}")
        else:
            print(f"  Skipping config file scan for first-time repository '{repo_name}'.")

        # Combine SBOM and config file CPEs
        all_cpes = sbom_cpes.union(config_cpes)
        print(f"  Total unique CPE count for '{repo_name}': {len(all_cpes)}")

        # Upsert repository record
        if record:
            repo_id = record[0]
            if record[1] != remote_commit:
                cur.execute("""
                    UPDATE repositories
                    SET clone_url = ?, last_commit = ?, last_updated = CURRENT_TIMESTAMP
                    WHERE id = ?
                """, (clone_url, remote_commit, repo_id))
                conn.commit()
                # send_slack_message(f":arrows_clockwise: *{repo_name}* updated with new commit `{remote_commit}`.")
        else:
            cur.execute("""
                INSERT INTO repositories (name, clone_url, last_commit)
                VALUES (?, ?, ?)
            """, (repo_name, clone_url, remote_commit))
            conn.commit()
            repo_id = cur.lastrowid
            # send_slack_message(f":new: New repository *{repo_name}* added with commit `{remote_commit}`.")

        # Update repository CPEs in DB
        if record:
            repo_id = record[0]
        else:
            cur.execute("SELECT id FROM repositories WHERE name = ?", (repo_name,))
            row = cur.fetchone()
            repo_id = row[0] if row else None

        if repo_id:
            update_repository_cpes(conn, repo_id, all_cpes)

        # Only delete the repository if we cloned it for a new SBOM (and it is not needed for config scanning)
        if needs_new_sbom and repo_path and os.path.exists(repo_path):
            delete_repository(repo_path, repo_name)

        time.sleep(REQUEST_DELAY)

    conn.close()
    print("[process_repositories] Completed.")

#############################
# Aggregation & Slack       #
#############################

def aggregate_vulnerabilities():
    conn = sqlite3.connect(DB_FILE)
    cur = conn.cursor()

    # Collect vulnerabilities from Grype
    cur.execute("""
        SELECT repository_name, vulnerability_id, actual_severity,
               predicted_severity, predicted_score, detailed_description,
               vulnerable_package, mitigation
        FROM grype_vulnerabilities
    """)
    grype_data = cur.fetchall()
    grype_vulns = []
    for g in grype_data:
        repo, vuln_id, actual, predicted, score, desc, pkg, mit = g
        grype_vulns.append({
            "repository": repo,
            "vulnerability_id": vuln_id,
            "actual_severity": actual,
            "predicted_severity": predicted,
            "predicted_score": score,
            "detailed_description": desc,
            "vulnerable_package": pkg,
            "mitigation": mit
        })

    # Collect vulnerabilities from NVD
    cur.execute("""
        SELECT nvd_vulnerabilities.repository_id, cve_id, cpe, vulnerability_json
        FROM nvd_vulnerabilities
    """)
    nvd_data = cur.fetchall()
    repo_lookup = {r[0]: r[1] for r in fetch_all_repository_records(conn)}
    nvd_vulns = []
    for row in nvd_data:
        repo_id, cve_id, cpe, vuln_json = row
        repo_name = repo_lookup.get(repo_id, "Unknown")
        nvd_vulns.append({
            "repository": repo_name,
            "vulnerability_id": cve_id,
            "actual_severity": "N/A",
            "predicted_severity": "N/A",
            "predicted_score": 0.0,
            "detailed_description": "",
            "vulnerable_package": "",
            "mitigation": ""
        })

    conn.close()

    all_vulns = grype_vulns + nvd_vulns
    sorted_vulns = sorted(all_vulns, key=lambda x: x.get("predicted_score", 0), reverse=True)
    with open(OUTPUT_AGGREGATED_FILE, "w") as f:
        json.dump(sorted_vulns, f, indent=4)
    return sorted_vulns

def send_aggregated_slack_file():
    aggregated = aggregate_vulnerabilities()
    if aggregated:
        msg = ":warning: *Aggregated Vulnerability Report* (Top high-priority items attached)"
        send_slack_file(OUTPUT_AGGREGATED_FILE, msg)
    else:
        send_slack_message("No new high-priority vulnerabilities detected.")

#############################
# Master Workflow           #
#############################

def run_main_workflow():
    # 1) Process repositories (SBOM + (if applicable) config scanning)
    process_repositories()

    # # 2) Grype Vulnerability Scan (run unconditionally on SBOMs)
    new_grype_vulns = process_grype_vulnerabilities()
    
    if new_grype_vulns:
    # Filter for high-scoring vulns
        high_score_vulns = [v for v in new_grype_vulns if v.get('predicted_score', 0) >= 9]

    if high_score_vulns:
        high_score_vulns = sorted(high_score_vulns, key=lambda x: x.get('predicted_score', 0), reverse=True)

        lines = [":siren-alert: High severity vulnerabilities from Grype (score > 9):"]
        for nv in high_score_vulns:
            lines.append(
                f"- Repo: {nv['repository']} | Vuln: {nv['vulnerability_id']} | "
                f"Score: {nv['predicted_score']} | Sev: {nv['predicted_severity']}"
            )
        send_slack_message("\n".join(lines))

    # 3) NVD Feed Processing
    conn = init_db_all()
    new_nvd_count = process_nvd_feeds(conn)
    conn.close()
    if new_nvd_count:
        send_slack_message(f":warning:  New NVD vulnerabilities inserted: {new_nvd_count}")
    else:
        send_slack_message(":white_check_mark:  No new NVD vulnerabilities discovered.")

    # 4) Aggregate vulnerabilities and send as a single Slack file
    send_aggregated_slack_file()

    # send_slack_message("```All scanning & analysis complete. See DB and OpenAI logs for details.```")

#############################
# Main Entry Point          #
#############################

def main():
    os.makedirs(REPOS_DIR, exist_ok=True)
    os.makedirs(SBOMS_DIR, exist_ok=True)
    send_slack_message(":rocket: Tech Vulnerability Scanner started.")
    # Ensure DB tables exist (for resume functionality)
    temp_conn = init_db_all()
    temp_conn.close()
    run_main_workflow()

if __name__ == "__main__":
    main()
